import os
import zmq
import time
import pandas as pd
import pickle5 as pickle
import threading
import SimpleITK as sitk

from typing import Tuple

import torch
import torch.nn as nn
from torchvision import transforms
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split

import torchmetrics
from torchmetrics import Accuracy
from focal_loss.focal_loss import FocalLoss

from torch_model import UNet3D, SimpleUNet3D

# Reference perplexity.ai for pytorch 3D UNet skull strip seg model
# https://www.perplexity.ai/search/0df235a1-27ba-4b67-bf7b-89c2500685c7?s=u

# Also reference perplexity.ai for pytorch train 3D UNet skull strip seg model
# https://www.perplexity.ai/search/cf74b6d5-9888-462b-9063-e90859bbf389
# Refer to section on "MRIDataset(Dataset)"

# Getting two columns from nift csv data pd table
# nifti_csv_data["intensity_norm"] = normalized raw_index
# nifti_csv_data["mask_index"]

def iou_score(pred, target):
    intersection = torch.logical_and(pred, target).sum()
    union = torch.logical_or(pred, target).sum()
    iou = intersection / (union + 1e-7)
    return iou

class BrainMRIDataset(torch.utils.data.Dataset):
    def __init__(self, brain_voxel_list, brain_mask_list):
        self.voxel_paths = brain_voxel_list
        self.mask_paths = brain_mask_list

    def __len__(self):
        return len(self.voxel_paths)

    def __getitem__(self, idx):
        print("idx = {}".format(idx))

        # sitk to torch tensor dims (channels, depth, height, width)
        print("self.voxel_paths[idx] = {}".format(self.voxel_paths[idx]))
        voxel = sitk.ReadImage(self.voxel_paths[idx])
        voxel_array = sitk.GetArrayFromImage(voxel)
        voxel_tensor = torch.tensor(voxel_array).float()
        print("voxel_tensor shape = {}".format(voxel_tensor.shape))

        print("self.mask_paths[idx] = {}".format(self.mask_paths[idx]))
        mask_voxel = sitk.ReadImage(self.mask_paths[idx])
        mask_voxel_array = sitk.GetArrayFromImage(mask_voxel)
        mask_voxel_tensor = torch.from_numpy(mask_voxel_array).float()
        print("mask_voxel_tensor shape = {}".format(mask_voxel_tensor.shape))
        
        return voxel_tensor, mask_voxel_tensor

# Simple Training for SimpleUNet3D function
def simple_train_unet3d(nifti_csv_data, epochs=3):
    X_train, X_val, y_train, y_val = train_test_split(nifti_csv_data["intensity_norm"].tolist(), nifti_csv_data["mask_index"].tolist(), test_size=0.1)

    print("X_train len = {}".format(len(X_train)))
    print("X_val len = {}".format(len(X_val)))

    print("Creating brain train & val datasets")
    
    brain_train_dataset = BrainMRIDataset(X_train, y_train)
    brain_val_dataset = BrainMRIDataset(X_val, y_val)

    print("brain_train_dataset len = {}".format(len(brain_train_dataset)))
    print("brain_val_dataset len = {}".format(len(brain_val_dataset)))

    print("Creating brain train & val dataloaders")

    train_loader = DataLoader(brain_train_dataset, batch_size=2, shuffle=True, num_workers=4)
    val_loader = DataLoader(brain_val_dataset, batch_size=2, shuffle=False)

    print("Creating Skull Strip Seg UNet3D model")

    # in_channels=1 for 1 medical image modality T2-weighted; out_channels=1 for skull vs non-skull
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    unet3d_model = SimpleUNet3D(in_channels=1, out_channels=1).to(device)
    # unet3d_model = SimpleUNet().to(device)

    print("Compiling UNet3D with Adam, FocalLoss, IoU, Accuracy")

    # compile model with Adam optimizer, focal loss, metrics
    bce_criterion = nn.BCEWithLogitsLoss().to(device)
    optimizer = optim.Adam(unet3d_model.parameters(), lr=0.001)

    # Define desired variables for tracking best validation performance
    best_val_loss = float("inf")

    print("Training UNet3D across {} epochs".format(epochs))

    unet3d_model.train()
    step = 100
    for epoch in range(epochs):
        print("Epoch {}: Train across batch_idx and brain_data and target from train_loader".format(epoch))
        for batch_idx, (brain_data, target) in enumerate(train_loader):
            brain_data = brain_data.to(device)
            target = target.to(device)
            print("batch_idx = {}; brain_data len = {}; target len = {}".format(batch_idx, len(brain_data), len(target)))
            
            optimizer.zero_grad()
            outputs = unet3d_model(brain_data.unsqueeze(1))
            loss = bce_criterion(outputs, target.unsqueeze(1))
            loss.backward()
            optimizer.step()

        print("Running Evaluation")

        # validation
        unet3d_model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for val_brain_data, val_target in val_loader:
                val_brain_data = val_brain_data.to(device)
                val_target = val_target.to(device)

                val_output = unet3d_model(val_brain_data.unsqueeze(1))
                val_loss += bce_criterion(val_output, val_target.unsqueeze(1))
        
        # Calculate average validation loss and IoU score
        val_loss /= len(val_loader)

        # Update best validation performance and save model if it improves
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(unet3d_model.state_dict(), "best_unet3d_model_loss_{}.pt".format(step))

        print(f"--------------------Epoch {epoch+1}: Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}")

        step += 100

        unet3d_model.train()



def advanced_train_unet3d(nifti_csv_data, epochs=3):
    X_train, X_val, y_train, y_val = train_test_split(nifti_csv_data["intensity_norm"].tolist(), nifti_csv_data["mask_index"].tolist(), test_size=0.1)

    print("X_train len = {}".format(len(X_train)))
    print("X_val len = {}".format(len(X_val)))

    print("Creating brain train & val datasets")
    
    # train_transform = val_transform = transforms.Compose([
    #     transforms.ToTensor()
    # ])

    # brain_train_dataset = BrainMRIDataset(X_train, y_train, transform=train_transform)
    # brain_val_dataset = BrainMRIDataset(X_val, y_val, transform=val_transform)

    brain_train_dataset = BrainMRIDataset(X_train, y_train)
    brain_val_dataset = BrainMRIDataset(X_val, y_val)

    print("brain_train_dataset len = {}".format(len(brain_train_dataset)))
    print("brain_val_dataset len = {}".format(len(brain_val_dataset)))

    print("Creating brain train & val dataloaders")

    # Higher batch_size, we lose gpu memory
    train_loader = DataLoader(brain_train_dataset, batch_size=2, shuffle=True)
    val_loader = DataLoader(brain_val_dataset, batch_size=2, shuffle=False)

    print("Creating Skull Strip Seg UNet3D model")

    # in_channels=1 for 1 medical image modality T2-weighted; out_channels=1 for skull vs non-skull
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    unet3d_model = SimpleUNet3D(in_channels=1, out_channels=1).to(device)

    print("Compiling UNet3D with Adam, FocalLoss, IoU, Accuracy")

    # compile model with Adam optimizer, focal loss, metrics
    optimizer = optim.Adam(unet3d_model.parameters(), lr=1e-1)
    # focal_criterion = FocalLoss(gamma=0.7).to(device)
    # Use pytorch's built-in focal loss
    bce_criterion = nn.BCEWithLogitsLoss().to(device)
    accuracy = torchmetrics.Accuracy(task="binary").to(device)
    # (num_classes=2 for skull vs non-skull
    # iou_score = torchmetrics.IoU(num_classes=2)
    metrics = [iou_score, accuracy]

    # Define desired variables for tracking best validation performance
    best_val_loss = float("inf")
    best_val_iou = float("-inf")

    print("Training UNet3D across {} epochs".format(epochs))

    unet3d_model.train()
    step = 100
    for epoch in range(epochs):
        print("Epoch {}: Train across batch_idx and brain_data and target from train_loader".format(epoch))
        for batch_idx, (brain_data, target) in enumerate(train_loader):
            brain_data = brain_data.to(device)
            target = target.to(device)
            print("batch_idx = {}; brain_data len = {}; target len = {}".format(batch_idx, len(brain_data), len(target)))
            
            optimizer.zero_grad()
            output = unet3d_model(brain_data.unsqueeze(1))
            # output = torch.sigmoid(output) # only for raw focal_loss, ensure predicted values are within range of 0 and 1
            loss = bce_criterion(output, target.unsqueeze(1))
            train_iou = iou_score(output, target.unsqueeze(1))
            loss.backward()
            optimizer.step()

        print("Running Evaluation")

        # validation
        unet3d_model.eval()
        val_loss = 0.0
        val_iou = 0.0
        with torch.no_grad():
            for val_brain_data, val_target in val_loader:
                val_brain_data = val_brain_data.to(device)
                val_target = val_target.to(device)

                val_output = unet3d_model(val_brain_data.unsqueeze(1))
                # val_output = torch.sigmoid(val_output)
                val_loss += bce_criterion(val_output, val_target.unsqueeze(1))
                val_iou += iou_score(val_output, val_target.unsqueeze(1))
        
        # Calculate average validation loss and IoU score
        val_loss /= len(val_loader)
        val_iou /= len(val_loader)

        # Update best validation performance and save model if it improves
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(unet3d_model.state_dict(), "best_unet3d_model_loss_{}.pt".format(step))

        if val_iou > best_val_iou:
            best_val_iou = val_iou
            torch.save(unet3d_model.state_dict(), "best_unet3d_model_iou_{}.pt".format(step))

        print(f"--------------------Epoch {epoch+1}: Train Loss: {loss:.4f} | Train IoU: {train_iou: .4f} | Val Loss: {val_loss:.4f} | Val IoU: {val_iou:.4f}")

        step += 100

        unet3d_model.train()

# References:
# - ChatGPT: https://chat.openai.com/c/78252db9-df19-429d-8473-c2b5a46c38e5

# Reference perplexity.ai for logic where subscriber sends confirmation to publisher when message is received:
# https://www.perplexity.ai/search/cf3ca7f0-a7f2-4395-8952-67198b09aef7?s=u

class ZMQSubscriber:
    def __init__(self, zmq_socket_address="tcp://127.0.0.1:5556"):
        self.zmq_socket_address = zmq_socket_address
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.SUB)
        self.socket.connect(self.zmq_socket_address)
        self.socket.setsockopt_string(zmq.SUBSCRIBE, "")
        self.running = False
        self.receive_thread = None
        self.data_to_process = None # store received data

    def start(self):
        self.running = True
        self.receive_thread = threading.Thread(target=self.receive_data)
        self.receive_thread.start()

    def stop(self):
        self.running = False
        if self.receive_thread:
            self.receive_thread.join()

    def receive_data(self):
        while self.running:
            try:
                # Receive the DataFrame bytes
                dataframe_bytes = self.socket.recv()

                # Deserialize the DataFrame using pickle
                received_dataframe = pickle.loads(dataframe_bytes)

                print("received_dataframe.head = {}".format(received_dataframe.head()))

                self.data_to_process = received_dataframe

                print("Going to send confirmation to NiFi PutZMQ Publisher")
                self.socket.send_string("Received")
            except Exception as e:
                print("Error receiving DataFrame: {}".format(e))

    def __del__(self):
        self.stop()
        self.socket.close()

def main():
    zmq_subscriber = ZMQSubscriber()

    nifti_csv_df = None
    epochs = 7

    try:
        print("Subscribing to {} to receive SkullStripSeg Prep DataFrame...".format(zmq_subscriber.zmq_socket_address))
        zmq_subscriber.start()

        while True:
            # check if there is data to process
            if zmq_subscriber.data_to_process is not None:
                nifti_csv_df = zmq_subscriber.data_to_process

                # pass the data to the training thread
                train_thread = threading.Thread(target=train_skull_strip_seg_model, args=(nifti_csv_df, epochs))
                train_thread.start()

                # reset the data_to_process to None to indicate processing is done
                zmq_subscriber.data_to_process = None
                print("Received data filepaths in pd df to train, finished training, exiting")
                break

            time.sleep(0.2) # sleep for 200 milliseconds

    except KeyboardInterrupt:
        print("Stopping the subscriber...")
        zmq_subscriber.stop()

def adv_skull_strip_seg_unet_training():
    nifti_csv_df = pd.read_csv("nfbs_skull_strip_seg_prep.csv")
    advanced_train_unet3d(nifti_csv_df, epochs=3)

def simple_skull_strip_seg_unet_training():
    nifti_csv_df = pd.read_csv("nfbs_skull_strip_seg_prep.csv")
    simple_train_unet3d(nifti_csv_df, epochs=2)

def test_unet():
    # Note: I get an out of gpu memory issue if I use gpu device, it was probably due to not properly handling batches
    # For now use CPU here
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    unet3d_model = SimpleUNet3D(in_channels=1, out_channels=1)

    batch_size = 4
    channels = 1
    depth = 96
    height = 128
    width = 160

    x = torch.randn((batch_size, channels, depth, height, width))

    preds = unet3d_model(x)

    print(preds.shape)
    print(x.shape)

    assert preds.shape == x.shape

if __name__ == "__main__":
    # test_unet()
    # simple_skull_strip_seg_unet_training()
    adv_skull_strip_seg_unet_training()